<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="PAPER_TITLE - AUTHOR_NAMES">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="KEYWORD1, KEYWORD2, KEYWORD3, machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="FIRST_AUTHOR_NAME, SECOND_AUTHOR_NAME">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="INSTITUTION_OR_LAB_NAME">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PAPER_TITLE">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>IPFormer [NeurIPS 2025]</title>
  
  <!-- Favicon and App Icons -->
<!--   <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico"> -->
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- KaTeX for LaTeX math rendering -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>



  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">IPFormer: Visual 3D Panoptic Scene Completion with Context-Adaptive Instance Proposals</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://markus-42.github.io/" target="_blank">Markus Gross</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/aya-fahmy-7373441bb/" target="_blank">Aya Fahmy</a><sup>1</sup>,
              </span>
              <span class="author-block">
                 <a href="https://www.linkedin.com/in/danit-niwattananan-50b4321a6/" target="_blank">Danit Niwattananan</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://dominikmuhle.github.io/" target="_blank">Dominik Muhle</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://rruisong.github.io/" target="_blank">Rui Song</a><sup>1,2</sup>
              </span>              
              <span class="author-block">
                <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=7Qdm9jUAAAAJ&hl=en" target="_blank">Henri Meeß</a><sup>1</sup>
              </span>                                        
            </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block">
                      <!-- Institution columns with names and logos -->
                      <div style="display: flex; justify-content: center; align-items: flex-start; gap: clamp(40px, 8vw, 80px); margin: 10px 0; flex-wrap: wrap;">
                        <!-- Column 1: Fraunhofer -->
                        <div style="text-align: center; flex: 0 0 auto; min-width: 0;">
                          <div><sup>1 </sup>Fraunhofer Institute IVI</div>
                          <img src="static/images/fraunhofer_logo.png" alt="Fraunhofer Institute" style="height: 50px; width: auto; margin-top: 8px; max-width: 100%; object-fit: contain; flex-shrink: 0;">
                        </div>
                        <!-- Column 2: TUM -->
                        <div style="text-align: center; flex: 0 0 auto; min-width: 0;">
                          <div><sup>2 </sup>Technical University of Munich</div>
                          <img src="static/images/tum_logo.png" alt="Technical University of Munich" style="height: 50px; width: auto; margin-top: 8px; max-width: 100%; object-fit: contain; flex-shrink: 0;">
                        </div>
                      </div>
                      
                      <br><i><span style="color: magenta;">Neural Information Processing Systems (NeurIPS) 2025</span></i>
                    </span>
                    <!-- TODO: Remove this line if no equal contribution -->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2506.20671.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper + Appendix</span>
                      </a>
                    </span>

                    <!-- TODO: Add your supplementary material PDF or remove this section -->
<!--                     <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/markus-42/ipformer" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Intro video-->
<section class="hero">
  <div class="container is-max-desktop">
    <div class="hero-body" style="padding-top: clamp(-6rem, -3vw, -6rem);">
      <div style="text-align: center;">
        <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata" style="width: 80%; height: auto;">
          <source src="static/videos/ipformer_intro.mp4" type="video/mp4">
        </video>
      <!-- TODO: Replace with your video description -->
      <h2 class="subtitle has-text-centered has-text-justified">
        <b>Visual Panoptic Scene Completion:</b> Using camera images, infer the complete 3D structure of a scene as a voxel grid, including both visible and occluded regions. Every voxel carries (1) binary <b>occupancy</b>, (2) a <b>semantic</b> label, and (3) an <b>instance</b> ID to group countable objects.
      </h2>
      </div>
  </div>
</section>
<!-- End teaser video -->


<!-- Paper abstract - NORMAL FULL TEXT -->
<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Semantic Scene Completion (SSC) has emerged as a pivotal approach for jointly learning scene geometry and semantics, enabling downstream applications such as navigation in mobile robotics.
            The recent generalization to Panoptic Scene Completion (PSC) advances the SSC domain by integrating instance-level information, thereby enhancing object-level sensitivity in scene understanding.
            While PSC was introduced using LiDAR modality, methods based on camera images remain largely unexplored.
            Moreover, recent Transformer-based approaches utilize a fixed set of learned queries to reconstruct objects within the scene volume.
            Although these queries are typically updated with image context during training, they remain static at test time, limiting their ability to dynamically adapt specifically to the observed scene.
            To overcome these limitations, we propose IPFormer, the first method that leverages context-adaptive instance proposals at train and test time to address vision-based 3D Panoptic Scene Completion. 
            Specifically, IPFormer adaptively initializes these queries as panoptic instance proposals derived from image context and further refines them through attention-based encoding and decoding to reason about semantic instance-voxel relationships.
            Extensive experimental results show that our approach achieves state-of-the-art in-domain performance, exhibits superior zero-shot generalization on out-of-domain data, and achieves a runtime reduction exceeding 14x.
            These results highlight our introduction of context-adaptive instance proposals as a pioneering effort in addressing vision-based 3D Panoptic Scene Completion.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End paper abstract -->


<!-- Paper abstract - TOGGL TO EXPAND FULL TEXT -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="abstract-container">
          <input type="checkbox" id="abstract-toggle" class="abstract-toggle">
          <div class="content has-text-justified abstract-content">
            <p>
              Semantic Scene Completion (SSC) has emerged as a pivotal approach for jointly learning scene geometry and semantics, enabling downstream applications such as navigation in mobile robotics.
              The recent generalization to Panoptic Scene Completion (PSC) advances the SSC domain by integrating instance-level information, thereby enhancing object-level sensitivity in scene understanding.
              While PSC was introduced using LiDAR modality, methods based on camera images remain largely unexplored.
              Moreover, recent Transformer-based approaches utilize a fixed set of learned queries to reconstruct objects within the scene volume.
              Although these queries are typically updated with image context during training, they remain static at test time, limiting their ability to dynamically adapt specifically to the observed scene.
              To overcome these limitations, we propose IPFormer, the first method that leverages context-adaptive instance proposals at train and test time to address vision-based 3D Panoptic Scene Completion. 
              Specifically, IPFormer adaptively initializes these queries as panoptic instance proposals derived from image context and further refines them through attention-based encoding and decoding to reason about semantic instance-voxel relationships.
              Extensive experimental results show that our approach achieves state-of-the-art in-domain performance, exhibits superior zero-shot generalization on out-of-domain data, and achieves a runtime reduction exceeding 14$\times$.
              These results highlight our introduction of context-adaptive instance proposals as a pioneering effort in addressing vision-based 3D Panoptic Scene Completion.              <!-- Your full abstract continues here... -->
            </p>
          </div>
          <label for="abstract-toggle" class="abstract-toggle-btn">
            <span class="expand-text">Show more</span>
            <span class="collapse-text">Show less</span>
            <i class="fas fa-chevron-down arrow"></i>
          </label>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Image: Challenge & solution -->
<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <h2 class="title is-3">Challenge & Solution</h2>
        </div>
        <div class="columns is-centered has-text-centered">
          <img src="static/images/IPF_vs_Symphonies_projPage.png" alt="IPformer (ours) vs. Symphonies" style="width: 50%; height: auto;" loading="lazy"/>
        </div>
        <h2 class="subtitle has-text-centered has-text-justified">
          Previous methods (1) only infer <b>occupancy</b> and <b>semantics</b> in an end-to-end fashion (Semantic Scene Completion), but require subsequent, time-consuming Euclidean clustering to retrieve individual <b>instances</b>.
          (2) they reconstruct objects using a fixed set of learned queries that are updated with image context during training, but remain static at test time and thus fail to dynamically adapt specifically to the observed scene.
          To address these challanges, our method (1) infers <b>occupancy</b>, <b>semantics</b>, and <b>instances</b> in an end-to-end fashion, and (2) initializes object queries from image context, referred to as instance proposals, which dynamically adapt specifically to the observed scene at train and test time.

      </h2>
    </div>
</section>
<!-- End image-->



<!-- Image: Saliency Analysis -->
<section class="hero is-light">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <h2 class="title is-3">Context-Adaptivity of Instance Proposals</h2>
        </div>
        <div class="columns is-centered has-text-centered">
          <img src="static/images/saliency_reb.png" alt="Context-Adaptivity of Instance Proposals" style="width: 100%; height: auto;" loading="lazy"/>
        </div>
        <h2 class="subtitle has-text-centered has-text-justified">
          <b>Instance-specific saliency.</b> 
          Through gradient-based attribution, we derive saliency maps that highlight image regions in green, where an individual instance mainly retrieves context from.
          Our introduced instance proposals effectively adapt to scene characteristics by guiding feature aggregation, substantially improving identification, classification, and completion.
          In contrast, non-adaptive instance queries sample context in an undirected manner, causing misclassification and geometric ambiguity.
      </h2>
    </div>
</section>
<!-- End image-->


<!-- Image: Architecture -->
<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <h2 class="title is-3">IPFormer Architecture</h2>
        </div>
        <div class="columns is-centered has-text-centered">
          <img src="static/images/IPF_architecture.png" alt="IPFormer architecture" style="width: 100%; height: auto;" loading="lazy"/>
        </div>
        <h2 class="subtitle has-text-centered has-text-justified">
          IPFormer refines image features and a depth map to produce 3D context features, which are sampled based on visibility to generate context-adaptive instance and voxel proposals.
          In a two-stage training strategy, voxel proposals first handle Semantic Scene Completion, effectively guiding the latent space toward detailed geometry and semantics.
          The second stage attends instance proposals over the pretrained voxel features to register individual instances. This design aligns occupancy, semantics, and instances, enabling robust Panoptic Scene Completion.
      </h2>
    </div>
</section>
<!-- End image-->



<!-- Images: Quantitative Results -->
<section class="hero is-light">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <h2 class="title is-3">Quantitative Results</h2>
        </div>
        <h2 class="subtitle has-text-centered has-text-justified">
          <b>In-domain performance</b> on SemanticKITTI val. set. 
          Best and second-best results are bold and underlined, respectively.
          Due to the absence of established baselines for vision-based PSC, we infer state-of-the-art SSC methods and apply DBSCAN to retrieve instances.
          In summary, IPFormer exceeds all baselines in overall panoptic metrics PQ and PQ$^{\dag}$, and achieves best or second-best results on individual metrics.
          Additionally, our method directly predicts a full panoptic scene, resulting in a significantly superior runtime of 0.33 seconds, thus providing a runtime reduction of over 14$\times$.
      </h2>
        <div class="columns is-centered has-text-centered">
          <img src="static/images/results_quant_in-domain.png" alt="In-Domain Quantitative Results" style="width: 100%; height: auto;" loading="lazy"/>
        </div>
        <h2 class="subtitle has-text-centered has-text-justified" style="padding-top: 2rem;">
          <b>Out-of-domain zero-shot generalization performance</b> of IPFormer and the closest baseline CGFormer+DBSCAN, by training on SemanticKITTI and cross-validating on the distinct SSCBench-KITTI360 test set.
          IPFormer demonstrates superior absolute and relative generalization performance across PSC and SSC metrics.
      </h2>
        <div class="columns is-centered has-text-centered">
          <img src="static/images/results_quant_out-domain.png" alt="Out-of-Domain Quantitative Results" style="width: 100%; height: auto;" loading="lazy"/>
        </div>
    </div>
</section>
<!-- End image-->



<!-- Image: Qualitative Results -->
<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <h2 class="title is-3">Qualitative Results</h2>
        </div>
        <div class="columns is-centered has-text-centered">
          <img src="static/images/results_qual.png" alt="Qualitative Results" style="width: 100%; height: auto;" loading="lazy"/>
        </div>
        <h2 class="subtitle has-text-centered has-text-justified">
          Qualitative results on the SemanticKITTI val. set (zoom in for best view). Each top row illustrates purely semantic information, following the SSC color map.
          Each bottom row displays individual instances, with randomly assigned colors to facilitate differentiation. Note that we specifically show instances of the Thing-category for clarity.
          IPFormer surpasses existing approaches by excelling at identifying individual instances, inferring their semantics, and reconstructing geometry with exceptional fidelity.
          Even for extremely low-frequency categories such as the person category (0.07%) under adverse lighting conditions, and in the presence of trace-artifacts from dynamic objects in the ground-truth data, our method proves visually superior.
          These advancements stem from IPFormer’s instance proposals, which dynamically adapt to scene characteristics, thus preserving high precision in instance identification, semantic segmentation, and geometric completion. 
      </h2>
    </div>
</section>
<!-- End image-->




<!-- Youtube video -->
<section class="hero is-light">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">IPFormer Full Explanation with Audio</h2>
      </div>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/KlXAvp-mIU4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Poster -->
<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <h2 class="title is-3">Poster</h2>
        </div>
        <div class="columns is-centered has-text-centered">
          <img src="static/images/poster_ipformer_4-3.png" alt="Poster" style="width: 100%; height: auto;" loading="lazy"/>
        </div>
    </div>
</section>
<!-- End image-->


<!-- Poster PDF-->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>
      <iframe  src="static/pdfs/Poster_IPFormer_NeurIPS_2025.pdf" width="100%" height="800">
          </iframe>
      </div>
    </div>
  </section> -->
<!--End paper poster -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>
        @inproceedings{gross2025ipformer,
        title={{IPF}ormer: Visual 3D Panoptic Scene Completion with Context-Adaptive Instance Proposals},
        author={Markus Gross and Aya Fahmy and Danit Niwattananan and Dominik Muhle and Rui Song and Daniel Cremers and Henri Meeß},
        booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS)},
        year={2025}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>




<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/carousel1.jpg" alt="First research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel2.jpg" alt="Second research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel3.jpg" alt="Third research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <img src="static/images/carousel4.jpg" alt="Fourth research result visualization" loading="lazy"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->



<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/carousel1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/carousel2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/carousel3.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->





<!-- Statcounter tracking code -->

  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  <!-- KaTeX JavaScript for math rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        delimiters: [
          {left: '$$', right: '$$', display: true},
          {left: '$', right: '$', display: false}
        ],
        throwOnError : false
      });
    });
  </script>

  </body>
  </html>
